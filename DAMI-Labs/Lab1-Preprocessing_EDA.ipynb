{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 1: EDA, Pre-processing, and Dimensionality Reduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "**Outline:**\r\n",
    "\r\n",
    "1. Reading the file, Summary Statistics\r\n",
    "\r\n",
    "2. Missing Values and Imputation\r\n",
    "\r\n",
    "3. EDA (Plotting, Outliers, Highly correlated features)\r\n",
    "\r\n",
    "4. Standardization\r\n",
    "\r\n",
    "5. Principal Component Analysis\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Import libraries\r\n",
    "\r\n",
    "import pandas as pd \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "#download scikit-learn\r\n",
    "#We will use scikit-learn library during this course. Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning \r\n",
    "#pip install scikit-learn \r\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Reading the file and summary statistics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the file\r\n",
    "\r\n",
    "\r\n",
    "In this session we will use the cleveland, heart disease dataset.  \r\n",
    "[link]( https://archive.ics.uci.edu/ml/datasets/heart+disease) to the dataset with information. \r\n",
    "\r\n",
    "#Note:\r\n",
    "The header parameter specifys the Row number(s) to use as the column names, and the start of the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "#Note:\r\n",
    "\r\n",
    "The header attribute specifys the row number(s) to use as the column names, and the start of the data. \r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "data = pd.read_csv(\"datasets\\\\processed.cleveland.data\", header=None)\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Dataset Information as found in heart-disease.names**\r\n",
    "\r\n",
    "This database contains 14 attributes and 303 patients that might not or might have a heart disease diagnoses. \r\n",
    "\r\n",
    "There are both continuous and categorical attributes. \r\n",
    "\r\n",
    "For example, the dataset has the age of the patients, resting blood pressure, maximum heart rate for numerical features. \r\n",
    "While sex, chest pain type or number of major vessels are categorical features. \r\n",
    "\r\n",
    "The class label is categorical, consists of labels from 0 to 4 and refers to a diagnosis of a heart disease. \r\n",
    "Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "### Feature explanation\r\n",
    "- age = in years // numerical, continuous\r\n",
    "\r\n",
    "- sex    =   (0 is female, 1 is male) // categorical\r\n",
    "\r\n",
    "\r\n",
    "- cp     = chest pain type (1 -> typical angina,  2 -> atypical angina,  3 -> non-anginal, 4 -> asymptomatic) //categorical\r\n",
    "\r\n",
    "- trestbps = resting blood pressure//continuous\r\n",
    "\r\n",
    "- chol      = serum cholestral in mg/dl //continuous\r\n",
    "\r\n",
    "- fbs       = fasting blood sugar > 120 mg/dl is 1 otherwise 0 //categorical\r\n",
    " \r\n",
    "- restecg   = resting electrocardiographic result, 0 -> normal, 1 -> St-T wave abnormality, 2 -> probable or definite hypertropy//categorical\r\n",
    "\r\n",
    "- thalach   = maximum heart rate achieved//continuous\r\n",
    "\r\n",
    "- exang     = exercise induced angina (1 = yes, 0 = no)//categorical\r\n",
    "\r\n",
    "- oldpeak   = ST depression induced by exercise relative to rest//continuous\r\n",
    "\r\n",
    "- slope     = the slope of the peak exercise ST segment (1 -> upslopping, 2 -> flat, 3 -> downslopping)//categorical\r\n",
    "\r\n",
    "- ca        = number of major vessels (0-3) covered by flourosopy//categorical\r\n",
    "\r\n",
    "- thal      = (3 -> normal, 6 -> fixed defect, 7 -> reversible defect)//categorical\r\n",
    "\r\n",
    "- class     = diagnosis of heart disease//categorical"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Change the column/feture names\r\n",
    "\r\n",
    "The name of the columns and generally information about this dataset can be found under the heart-disease.names file. \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "set_axis assigns a  desired index to given axis. \r\n",
    "Input:\r\n",
    "    a list-like index\r\n",
    "    the axis to update\r\n",
    "    inplace: whether to return a new dataframe instance\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "data.set_axis(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'class'], axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To summarize: \r\n",
    "\r\n",
    "- numerical are: age, trestbps, chol, thalac, oldpeak\r\n",
    "- categorical are: sex, cp, fbs, restecg, exang, slope, thal, class\r\n",
    "- our class/target is the column class\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Information and summary statistics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## you can see the first rows of the dataset by calling the head() method and inside there pass a small number of rows\r\n",
    "data.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#check number of rows(patients) and colums(features)\r\n",
    "data.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "You can print information about the dataset by using the info() method. \r\n",
    "\r\n",
    "We can see that the attributes ca and thal are of object type.\r\n",
    "\r\n",
    "A data type is essentially an internal construct that a programming language uses to understand how to store and manipulate data. \r\n",
    "- int: for integer numbers\r\n",
    "- float: for floating point numbers\r\n",
    "- object: for text or mixed numeric and non-numeric values\r\n",
    "\r\n",
    "Data types are one of those things that you don’t tend to care about until you get an error or some unexpected results. \r\n",
    "It is also one of the first things you should check once you load a new data into pandas for further analysis.\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "print('Data Show Info\\n')\r\n",
    "data.info()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Data Show Describe\\n')\r\n",
    "data.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# and finally the columns names can be accessed like:\r\n",
    "data.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Missing Values and Imputation\r\n",
    "\r\n",
    "Missing values: when no data are stored (due to some failure to record them or maybe data corruption).\r\n",
    "We need to handle them, as many machine learning algorithms do not support missing values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking for null values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Missing values can be represented as null values, or some random string like '?', '_', essentially when no data are stored. \r\n",
    "\r\n",
    "\r\n",
    "isnull() will return in every cell of the df True if the value is Null or False if it's not. \r\n",
    "if on that you additionally call any(),  it will summarize the results along a column \r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "# checking for null values\r\n",
    "# To detect NaN values pandas uses either .isna() or .isnull()\r\n",
    "\r\n",
    "data.isnull().any()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking for missing values in other forms\r\n",
    "\r\n",
    "Missing values might be written like: \"missing\", \"?\", \"_\", etc.\r\n",
    "We have noticed that that the attributes, ca and thal are of object type which means they might have mixed types. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "The above say that there are no missing values. Remember that they only look for None and NaN values. \r\n",
    "\r\n",
    "We have noticed that that the attributes, ca and thal are of object type which means they may have mixed types. \r\n",
    "Let's see what is unique about them as compared to the others!\r\n",
    "\r\n",
    "Rule of thumb! ALWAYS open the dataset file and investigate the data yourself and look for inconsistencies. \r\n",
    "Missing values might be written like: \"missing\", \"?\", \"_\", etc.\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "print(data['ca'].unique())\r\n",
    "print(data['thal'].unique())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How to handle the missing values\r\n",
    "\r\n",
    "In general,  there are different ways of dealing with missing values\r\n",
    "\r\n",
    "For example:\r\n",
    "\r\n",
    " \r\n",
    "1. if the feature is nominal (categorical), replace missing with most frequent category\r\n",
    "2. if feature is numeric, replace it with the mean value\r\n",
    "3. or simply delete the rows that have missing values if there is an insignificant amount of them (The simplest approach for dealing with missing values is to remove entire predictor(s) and/or sample(s) that contain missing values.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How many missing values per feature?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"ca: \", (data['ca'] == \"?\").sum())\r\n",
    "print(\"thal: \", (data['thal'] == \"?\").sum())\r\n",
    "print(\"#################\")\r\n",
    "print(\"whole df/ per column:\\n\", (data == \"?\").sum(axis=0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Delete rows with missing values\r\n",
    "\r\n",
    "This dataset we will use for the rest of the lab"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#drop everything that contains ?\r\n",
    "#In case you have empty values in the form of nan: \r\n",
    "#dataframe.dropna(inplace=True)\r\n",
    "\r\n",
    "data_dropped = data[(data[\"ca\"] != '?') & (data[\"thal\"] != '?')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dropped"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imputation using the most frequent value with sklearn\r\n",
    "\r\n",
    "The missing values are just a few so we can simply delete the rows that contain missing values. For the rest of the assignment we will use the dataframe with the dropped missing values.  \r\n",
    "However, this is an example of how you can impute missing values using the SimpleImputer from sklearn. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#download sklearn first if you haven't already!\r\n",
    "\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "#first we define the object\r\n",
    "imputer = SimpleImputer(missing_values=\"?\", strategy='most_frequent', copy=False)#it works along the columns\r\n",
    "#then fit it to the data\r\n",
    "imputer.fit(data)\r\n",
    "\r\n",
    "data_imputed = imputer.transform(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Convert it to a dataframe\r\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data.columns)\r\n",
    "data_imputed\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "imputer.statistics_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Exploratory Data Analysis (EDA)\r\n",
    "Discover patterns, spot anomalies, check assumptions with the help of summary statistics,  graphical representations. \r\n",
    "\r\n",
    "\r\n",
    "Two types EDA:\r\n",
    "- Univariate: data being analyzed consists of just one variable. Histograms, Barplots, Boxplots (can be used for outliers) etc\r\n",
    "\r\n",
    "- Multivariate : investigate the relationship between two or more variables of the data through cross-tabulation or statistics.  Grouped bar plots, scatterplots, heatmaps for correlation etc\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How many patients are diagnosed with a heart disease? \r\n",
    "\r\n",
    "**Remember**:\r\n",
    "- presence of heart disease: values 1,2,3,4 \r\n",
    "- absence: value 0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#count_values(): returns a series containing counts of unique values\r\n",
    "\r\n",
    "data_dropped[\"class\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Balancing the problem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "\r\n",
    "There are 160 patients that do not have heart disease.\r\n",
    "While the patients that are postive to heart disease are distributed among labels 1 to 4.\r\n",
    "We can make the problem more balanced by transforming all these positive class labels to 1.\r\n",
    "\r\n",
    "This you can use later on in supervised learning to make your dataset more balanced. \r\n",
    "But now we do it as we will plot stuff and it's better and easier to visualize it like this!\r\n",
    "\r\n",
    "Use the replace method to map the values  2, 3, 4 to 1!\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "data_dropped[\"class\"].replace({2: 1, 3: 1, 4:1}, inplace=True)\r\n",
    "#alternative: data['class'] = np.where((data['num']>0),1,0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# let's count the values again\r\n",
    "data_dropped[\"class\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the class - barplot  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "The figure object is your empty canvas.\r\n",
    "\r\n",
    "plt.style.use('ggplot'): the  \"ggplot\" style,  adjusts the style to emulate ggplot (a popular plotting package for R).\r\n",
    "\r\n",
    "Barplot parameters: \r\n",
    "    x : a sequence of scalars (unique values of the class label)\r\n",
    "    y the height of the bars (how many samples each class has)\r\n",
    "\r\n",
    "set_xticks(): Set the x ticks with list of ticks\r\n",
    "    Essentialy here we will pass the names of the bars. \r\n",
    "\r\n",
    "grid(): adds a grid to the plot\r\n",
    "        b: Whether to show the grid lines. True/False\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "#first create a series where you will store the class label\r\n",
    "class_label = data_dropped[\"class\"]\r\n",
    "#replace the 0 with negative and 1 with positive, for the sake of interpretation\r\n",
    "class_label = class_label.replace({0: 'negative', 1: 'positive'})\r\n",
    "plt.style.use('ggplot')\r\n",
    "\r\n",
    "fig = plt.figure()\r\n",
    "ax = fig.add_subplot()\r\n",
    "\r\n",
    "ax.bar(class_label.unique(), class_label.value_counts())\r\n",
    "ax.set_title(\"Class/Target\")\r\n",
    "ax.set_xticks(class_label.unique())\r\n",
    "ax.grid(b=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check for outliers with boxplots\r\n",
    "\r\n",
    "Outliers can either be a mistake or just variance.\r\n",
    "\r\n",
    "To check for outliers we can use the boxplot to see the distribution of the attributes. \r\n",
    "Any outliers are normally outside the plot region"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Create boxplots for all the numerical features\r\n",
    "\r\n",
    "Instead of plt.figure you can call plt.subplots and specify how many rows and columns you want.\r\n",
    "if nrows=1, ncols=2 it will create 1 row with 2 plots/ if nrows=2, ncols=3 it will create 2 rows with 3 plots each.\r\n",
    "\r\n",
    "Note!\r\n",
    "\r\n",
    "The axes attribute is just a list of the matplotlib axes. So you can actually iterate through and create  different plots!\r\n",
    "\r\n",
    "\r\n",
    "tight_layout(): automatically adjusts subplot params so that the subplot(s) fits in to the figure area\r\n",
    "\r\n",
    "\r\n",
    "barplot:\r\n",
    "    x: The input data.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "fig, ax = plt.subplots(1, 4)\r\n",
    "\r\n",
    "plt.tight_layout()\r\n",
    "ax[0].boxplot(data_dropped['trestbps'])\r\n",
    "ax[0].set_title(\"trestbps\")\r\n",
    "\r\n",
    "ax[1].set_title('chol')\r\n",
    "ax[1].boxplot(data[\"chol\"])\r\n",
    "\r\n",
    "ax[2].set_title('thalach')\r\n",
    "ax[2].boxplot(data[\"thalach\"])\r\n",
    "\r\n",
    "ax[3].set_title('oldpeak')\r\n",
    "ax[3].boxplot(data[\"oldpeak\"])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouped bar-plot\r\n",
    "Count the values of males and females for each of the class labels and make a grouped box-plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Groupby: allows you to group together rows based on a column and perform an aggregate function on them. \r\n",
    "After groupby, specify a summarization function!\r\n",
    "\r\n",
    "To plot: \r\n",
    "Call unstack, that pivots the grouped dataframe back, and just call plot with kind equals to bar!\r\n",
    "\r\n",
    "stacked: The bars for the different class labels will be put one top of each other, instead of next to each other. \r\n",
    "Convert it to False if you want to see the difference\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "ylabels = ['negative', 'positive']\r\n",
    "labels = (\"female\", \"male\")\r\n",
    "\r\n",
    "positions = (0, 1)\r\n",
    "\r\n",
    "s_x = data_dropped.groupby(\"sex\")['class'].value_counts()\r\n",
    "s_x.unstack().plot(kind='bar', stacked= True)\r\n",
    "\r\n",
    "plt.legend(labels=ylabels)\r\n",
    "plt.xticks(positions, labels)\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#check how groupby and unstack works\r\n",
    "#for each female and male we have the counts of the class labels\r\n",
    "s_x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#if you call unstack the dataframe now is pivoted back!\r\n",
    "s_x.unstack()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouped bar-plot\r\n",
    "How many people are negative to heart disease and how many are posive, per age?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Same procedure: \r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "by_age= data_dropped.groupby([\"age\"])['class'].value_counts()\r\n",
    "by_age.unstack().plot(kind='bar', stacked=True)\r\n",
    "plt.legend(labels=ylabels)\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#similarly\r\n",
    "fbs_by_class = data_dropped.groupby(\"fbs\")['class'].value_counts()\r\n",
    "fbs_by_class.unstack().plot(kind='bar', stacked= False)\r\n",
    "plt.legend(labels=ylabels)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Boxplot with seaborn \r\n",
    "Age in relation to class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#boxplot of age in relation to the class\r\n",
    "\r\n",
    "f, ax = plt.subplots(figsize=(8, 6))\r\n",
    "sns.boxplot(x=\"class\", y=\"age\", data=data_dropped)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Highly correlated features and Heatmap\r\n",
    "\r\n",
    "A strong correlation is indicated by a Pearson Correlation Coefficient value near 1. Therefore, when looking at the Heatmap, we want to see what correlates most with the class label.\r\n",
    "\r\n",
    "\r\n",
    "— A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\r\n",
    "\r\n",
    "\r\n",
    "— A value closer to 1 implies stronger positive correlation\r\n",
    "\r\n",
    "\r\n",
    "— A value closer to -1 implies stronger negative correlation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Using Pearson Correlation\r\n",
    "\r\n",
    "plt.figure(figsize=(12,10))\r\n",
    "\r\n",
    "#Computes pairwise correlation of columns, excluding NA/null value. returns \r\n",
    "cor = data_dropped.corr()\r\n",
    "\r\n",
    "#Plots rectangular data as a color-encoded matrix. note that we are using seaborn. The parameter data needs to be rectangular dataset of pairwise correlations\r\n",
    "sns.heatmap(data=cor, annot=True, cmap=plt.cm.Reds);\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pairplot\r\n",
    "\r\n",
    "Invetigate pairwise relationships"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Pairplot from seaborn:  investigates pairwise relationships.\r\n",
    "\r\n",
    "Make a list of the numerical values, and in hue pass the class!\r\n",
    "\r\n",
    "\r\n",
    "On the diagonal you see the distribution of these diffeent numerical variables\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "sns.pairplot(data_dropped[['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'class']], hue='class', size=2.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary: Data Preprocessing steps\r\n",
    "\r\n",
    "1. Check and handle  Missing Values \r\n",
    "2. Check for outliers \r\n",
    "4. Check for highly correlated features \r\n",
    "5. Standardize or Normalize numeric features "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Standardization \r\n",
    "\r\n",
    "We only standardize the numerical features. Class should not be standardized either!\r\n",
    "\r\n",
    "\r\n",
    "Information about StandardScaler: [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\r\n",
    "\r\n",
    "Compare the effects of different scalers from sklearn: [link](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Standardization is a crucial step before performing PCA, since we are interested in the components that maximize the variance. \r\n",
    "\r\n",
    "fit: computes the mean and std to be used for later scaling. \r\n",
    "\r\n",
    "transform: uses a previously computed mean and std to autoscale the data \r\n",
    "\r\n",
    "fit_transform:  does both at the same time. So you can do it with 1 line of code instead of 2.\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "numerical = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"] #list of num features\r\n",
    "\r\n",
    "X = data_dropped[numerical]\r\n",
    "\r\n",
    "\r\n",
    "scaler = StandardScaler().fit(X)\r\n",
    "\r\n",
    "data_scaled = scaler.transform(X)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_scaled = pd.DataFrame(data_scaled, columns=numerical)\r\n",
    "data_scaled.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Principal Component Analysis (PCA)\r\n",
    "\r\n",
    "Overall PCA  attemps to find out what features explain the most variance in your data\r\n",
    "\r\n",
    "PCA also helps you visualize your data. \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import PCA\r\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Intatiate a pca object, specify how many principal components you want\r\n",
    "\r\n",
    "Fit your dataset\r\n",
    "Transform: and the apply the rotation and dimensionality reduction\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "pca = PCA(n_components=2)\r\n",
    "x_pca = pca.fit_transform(data_scaled)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#original shape\r\n",
    "data_scaled.shape\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#shape after PCA\r\n",
    "x_pca.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the principal components with colors on the class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "\r\n",
    "Scatterplot to plot the two principal components. \r\n",
    "\r\n",
    "Note:\r\n",
    "In general, interpreting the  components is not  easy. \r\n",
    "But based on the two components we can see if we have a clear seperation between the labels. \r\n",
    "\r\n",
    "For this dataset the seperation is not as clear. (in toy datasets you might be able to see clear groups forming)\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "plt.figure(figsize=(8,6))\r\n",
    "plt.scatter(x=x_pca[:,0], y=x_pca[:,1], c=data_dropped['class'], cmap='rainbow')\r\n",
    "plt.xlabel('1st PC')\r\n",
    "plt.ylabel('2nd PC')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explained variance ratio "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the explained variance for each component is stored in an attribure of the pca object called explained_variance_ratio_\r\n",
    "\r\n",
    "pca.explained_variance_ratio_\r\n",
    "\r\n",
    "# the first component explains 35% and the second 21%\r\n",
    "\r\n",
    "# Components are a linear transformation that chooses a variable system for the dataset such that the greatest variance of the dataset comes to lie on the first axis. \r\n",
    "# and likewise the second greatest variance lies on the second axis.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA components"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pca.components_\r\n",
    "\r\n",
    "# Each row represents a principal component and each column actually relates back to the original features\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_comp = pd.DataFrame(pca.components_,  columns=numerical)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_comp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Heatmap for components"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "If we create a heatmap now we will be able to see the correlation between various features and the principal components themselves\r\n",
    "\r\n",
    "In that way you can see which features are more important for each principal component \r\n",
    "\"\"\"\r\n",
    "\r\n",
    "plt.figure(figsize=(12,6))\r\n",
    "sns.heatmap(df_comp, cmap='plasma')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section we have discussed the use of principal component analysis for dimensionality reduction and for visualization of high-dimensional data. \r\n",
    "Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data.\r\n",
    "PCA's main weakness is that it tends to be highly affected by outliers in the data. For this reason, many robust variants of PCA have been developed, many of which act to iteratively discard data points that are poorly described by the initial components. Scikit-Learn contains a couple interesting variants on PCA, including RandomizedPCA and SparsePCA, both also in the sklearn.decomposition submodule"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# END OF LAB 1"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "interpreter": {
   "hash": "4a3821e50dfd29f54393a38062d93a54d0c9d954cd67861638d013f261604981"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}