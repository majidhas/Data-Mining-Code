{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 3: Supervised learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This lab shows how to run simple classification models using the scikit-learn library. In this lab, you will learn how to build different classification models on a given training set and then apply them to predict the classes on a test set. This lab will also show you how to derive accuracy from the test set, one of the most famous performance measures."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Contents\n",
    "- 3-1. Supervised learning models using scikit-learn\n",
    "  - Perceptron\n",
    "  - K-nearest neighbors\n",
    "  - Decision tree\n",
    "  - Support vector machines\n",
    "\n",
    "- 3-2. Manual implementation\n",
    "  - Perceptron\n",
    "  - K-nearest neighbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3-1. Supervised learning models  using scikit-learn "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may already know basic concepts of scikit-learn from the previous labs. We will keep the same format, but now our task is **supervised learning**, which means we now deal with the datasets with answers.\n",
    "\n",
    "To use scikit-learn, we do not need to rely on python syntax such as functions or classes; rather, we just load and call the methods provided by the library directly on the console.\n",
    "We will use **Connectionist Bench** from UCI Machine Learning Repository, which can be downloaded [here](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data). We already located the dataset into **datasets** directory, so you can also simply include it from there. This dataset has two classes: ***Mines***, ***Rocks*** with 60 attributes representing each data entity. More information can be found [here](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the libraries\n",
    "\n",
    "Basic libraries used throughout this lab session. Random seed is set to ensure the same results with the instructor's ones."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "RANDOM_SEED = 12345"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first thing you need to do is to load the data and check if it's correctly loaded. We will use a simple pandas method called **read_csv** to load csv-like datasets - datasets with a unique separator such as comma (,) or tab (   ). Since there is no header of the table in the dataset, you need to choose not to use the first row as a set of column names. \n",
    "\n",
    "* The dataset is located in the **datasets** directory and its name is **sonar.all-data**.\n",
    "\n",
    "* header parameter should be provided."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"datasets/sonar.all-data\", header = None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can always check whether the shape of data by looking at the first five rows using the **head** method.\n",
    "\n",
    "* DataFrame.head()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also check the null values. You can use .info() that you learned from the previous labs.\n",
    "\n",
    "* DataFrame.info()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.info()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is also isnull() function to check nulls in the dataframe.\n",
    "\n",
    "* isnull().sum() will return column-wise summation of `True`s.\n",
    "* isnull().sum().sum() will finally return how many nulls are in the datset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.isnull().sum().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our dataframe has both attributes (0-59) and labels (60) together in itself. However, scikit-learn requires that labels and data attributes should be separated. Let's separate the data labels from the dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = data.drop(60, axis=1)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = data.iloc[:, -1]\r\n",
    "y = data[60]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will split the dataset into two sets: training and test sets. Since we will not apply any validation strategy, such as k-fold cross-validation, splitting the whole dataset into two sets will be enough.\n",
    "\n",
    "To do this, we can manually pick some part of the data to create two different subsets. However, scikit-learn also provides one method for this job. We will use the **train_test_split** function in scikit_learn in the **model_selection** package."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method divides the entire dataset into a training set and a test set. To do this, we need to specify required parameters such as our data attributes (X) and labels (y) and what percent we want to have for the test set (test_size). This method also has optional parameters such as 1) whether we want to allow shuffling (shuffle), 2) random state (random_state), or 3) whether we want to keep the label's proportions when we divide the data (stratify).\n",
    "\n",
    "Here we will divide the training and test sets with a 70:30 ratio.\n",
    "\n",
    "* Specify X, y, test_size, random_state, stratify"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Perceptron"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first algorithm we are going to make is **Perceptron**. Perceptron is a binary classifier having one weight (w) and one bias (b) value $wâˆ™x+b$. You can also regard it as a single neuron classifier.\n",
    "\n",
    "* Scikit-learn has perceptron as its built-in function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Perceptron** is in the linear_model package of scikit_learn."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import Perceptron"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform analysis, we first need to make an instance by calling a class **Perceptron**. It receives few parameters, which can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html). Since we are trying a basic perceptron we learned from the lecture, we do not need to put all the parameters scikit-learn supports. We will not use any regularization or early-stopping here, but there are still some parameters we need to consider.\n",
    "\n",
    "- max_iter: Perceptron can converge or cannot converge; it depends on the dataset. So we can at least set some reasonable maximum iteration.\n",
    "- fit_intercept: Perceptron can have intercept (or bias) value or not. You can state it here (True/False).\n",
    "- tol: Since it is also possible that Perceptron is not converged forever, we can state a stopping criterion. The iteration will stop when loss > previous_loss - tol.\n",
    "- shuffle: We can shuffle the training data with each iteration."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can firstly create our instance with the following options:\n",
    "\n",
    "* maximum iteration = 100.\n",
    "* without shuffling.\n",
    "* without a tol value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppn = Perceptron(max_iter=100, tol=None, shuffle = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we already have prepared our training data (X_train, y_train), we can call **fit** function with those variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppn.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now our model has trained its weights and bias and this information is stored in our instance **ppn**. Now we can get a *test error* on our test set by calling the **score** method, and check the predicted labels by calling the **predict** method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppn.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppn.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we set the **tol** parameter, the algorithm might finish earlier than our maximum iteration. We can also check it as it is stored in *n_iter_* variable in our instance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppn.n_iter_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppn = Perceptron(max_iter=100, tol=0.1, shuffle = False)\r\n",
    "ppn.fit(X_train, y_train)\r\n",
    "ppn.n_iter_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### K-nearest neighbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next algorithm we will try is k-nearest neighbors (kNN). In scikit-learn, all methods and processes we need are entirely the same. The only change is when we create an instance because different models will have different parameters. We can find kNN in the **neighbors** package of scikit-learn."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "kNN is a simple algorithm having a small number of parameters. Detailed information can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). However, we will only focus on the number of neighbors now as it is a critical factor of the algorithm's performance. We can do some experiments by changing the parameter. Besides that, you can also change the distance function from Euclidean to something else (p), and also, you can put more weight on the closest neighbor if the case is the numerical prediction (weights). Supported distance measures and other parameter information can be found on the official page.\n",
    "\n",
    "* Create a new instance of KNeighborsClassifier with n_neighbors=3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After making the instance, we can train and test our algorithm in the same way as Perceptron. We can use **fit** for training, **score** to get test accuracy, and **predict** to get the predicted labels of the test dataset.\n",
    "- Fit the classifier to `X_train` and `y_train`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "neigh.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Return a classification score of the trained model on `X_test` and `y_test`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "neigh.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Print predicted labels of `X_test`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "neigh.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decision tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's deal with the **decision tree**. When running a decision tree using scikit-learn, the process after creating an instance is again the same as other classifiers. Therefore, the most important thing is understanding the parameters for each model to correctly create a new instance. You can find a normal decision tree in the **tree** package."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To just create one decision tree instance, we do not need to put any parameter as DecisionTreeClassifier has default options for every parameter it has. The parameters are used to constraint the tree by limiting the maximum depth or minimum samples to split. There is no very optimal set of parameters that can be applied to all cases, so we may need to optimize it by running further optimization techniques such as *grid search*, which we will look into in the next lab. Here we are going to use a normal decision tree without specifying any parameter. Due to its randomness inside, we still need to state random state.\n",
    "\n",
    "- Create a new instance of DecisionTreeClassifier with random_state=RANDOM_SEED"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dtc = DecisionTreeClassifier(random_state = RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After making the instance, we can train and test our algorithm in the same way as Perceptron. We can use **fit** for training, **score** to get test accuracy, and **predict** to get the predicted labels of the test dataset.\n",
    "\n",
    "- Fit the classifier to `X_train` and `y_train`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dtc.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Return a classification score of the trained model on `X_test` and `y_test`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dtc.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Print predicted labels of `X_test`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dtc.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Support vector machines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scikit-learn offers a variety of support vector machine algorithms: SVC, NuSVC, and LinearSVC. SVC is a basic form of support vector machine supporting various kernels, while LinearSVC forms a linear boundary without a kernel (You can find more [here](https://scikit-learn.org/stable/modules/svm.html)). NuSVC is similar to SVC, but the biggest feature of it is that we can adjust the number of support vectors. All these three classifiers are available in the **SVM** package, and in this lab, we will use SVC."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.svm import SVC"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SVC** has a lot of parameters, similar to the decision tree we have seen earlier. Many parameters are used to fine-tune the model. One important parameter here is **C**, a regularization factor. This value is an indicator of how much the training set of the SVM can cover. The larger C, the smaller SVM's margin area, which means that the training set's fitting ability to the training set becomes stronger than before. Therefore, it is important to find C that can give the right level of regularization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can plot the iris dataset by differing C. This lab includes a function from the scikit-learn user guide website, which shows the decision boundary of an SVC model. We can try to apply various C values and see the difference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Please import **plot_iris** function by running this block below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dami_dsv.supervised_learning.plot_iris import plot_iris"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can freely change the value of C and see differences."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_iris(C=100.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_iris(C=5.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_iris(C=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can apply SVM to our dataset. There are many available kernels that scikit-learn supports, but we will use the RBF kernel, set as a default in scikit-learn. We will have a chance to deal with other kernels in the upcoming assignment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc = SVC(gamma=\"scale\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Fit the classifier to `X_train` and `y_train`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Return a classification score of the trained model on `X_test` and `y_test`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Print predicted labels of `X_test`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can change C and see the difference in the test score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc2 = SVC(C = 5, gamma=\"scale\")\r\n",
    "svc2.fit(X_train, y_train)\r\n",
    "svc2.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we give too much space for the margin, SVM can be underfitted and lose enough classification power. Too much generalization cannot always be good."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc3 = SVC(C = 0.1, gamma=\"scale\")\r\n",
    "svc3.fit(X_train, y_train)\r\n",
    "svc3.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3-2. Manual implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it is time to implement some algorithms we tried in this lab manually. It will give you a more robust understanding of the algorithm. We are going to implement simple ones: **perceptron** and **kNN**.\n",
    "\n",
    "In those implementations, we use the **class** notation and **self** variables inside. This structure is made to give you the same experience with scikit-learn when testing. You only use **self** here to call the methods defined in the class structure or to access the class variable defined by self inside the class. The class-based structure will not appear in the assignment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Perceptron"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before implementing perceptron, we need to change the letter classes into numbers as perceptron assumes that it receives binary numeric classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train_numeric = y_train.replace(['M', 'R'], [0, 1])\r\n",
    "y_test_numeric = y_test.replace(['M', 'R'], [0, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we already have a basic structure of our new perceptron classifier! It has the same structure with scikit-learn's one, so we can test our model in the same way as perceptron in Pandas after finishing the development."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Perceptron():\r\n",
    "    def __init__(self, max_iter):\r\n",
    "        \"\"\"\r\n",
    "        A constructor that receives parameters and save them into member variables.\r\n",
    "        You will receive max_iter value and need to save into self.max_iter.\r\n",
    "\r\n",
    "        Input:\r\n",
    "          max_iter: The maximum iteration of the algorithm.\r\n",
    "        Output:\r\n",
    "          None.\r\n",
    "        \"\"\"\r\n",
    "        return\r\n",
    "    \r\n",
    "    def fit(self, X, y):\r\n",
    "        \"\"\"\r\n",
    "        A method to train the model by receiving the training dataset and labels.\r\n",
    "        \r\n",
    "        Input:\r\n",
    "          X: Training dataset.\r\n",
    "          y: Training labels.\r\n",
    "\r\n",
    "        Output:\r\n",
    "          None.\r\n",
    "          \r\n",
    "        \"\"\"    \r\n",
    "        #- Step 1: The algorithm needs to set an empty list of size |attributes|+1 to save our weights (vector w) and bias (b).\r\n",
    "        #          The additional value is used for intercept (or bias) value of the perceptron classifier.\r\n",
    "        self.w = None\r\n",
    "        \r\n",
    "        #- Step 2: The algorithm iterates self.max_iter times and train the model.\r\n",
    "        for _ in range(None):\r\n",
    "        \r\n",
    "        #- Step 3: For each iteration, we traverse all rows in our dataset and predict the label of each row \r\n",
    "        #          by calling self.predict method.\r\n",
    "        #          We can calculate the 'error' to check whether our prediction was correct or not,\r\n",
    "        #          by substracting a predicted label from a true label.\r\n",
    "        #          \r\n",
    "            for _, _ in None:\r\n",
    "                prediction = None\r\n",
    "                error = None\r\n",
    "        \r\n",
    "        #- Step 4: When prediction was wrong, we update the weights by adding [error*row] to the previous weights. \r\n",
    "        #          For intercept value, we update it by simply adding the error to the previous value.\r\n",
    "        #          Assign the values to self.w so we can use the updated version in the next iteration.\r\n",
    "        \r\n",
    "                self.w[0] = self.w[0] + error\r\n",
    "                self.w[1:] = self.w[1:] + error * row\r\n",
    "        \r\n",
    "        return\r\n",
    "                \r\n",
    "    def predict(self, d1):\r\n",
    "        \"\"\"\r\n",
    "        A method to predict a label with trained weights.\r\n",
    "\r\n",
    "        Input:\r\n",
    "          row: A single row from dataset.\r\n",
    "        Output:\r\n",
    "          Binary integer (0 or 1).\r\n",
    "        \"\"\"\r\n",
    "        \r\n",
    "        #- Step 1: We calculate the dot product of our weights (self.w) and the given row.\r\n",
    "        #          For the bias value, we multiply this value by one since we do not have any value in the received row.\r\n",
    "        \r\n",
    "        act = self.w[0]\r\n",
    "        act += self.w[1:].dot(row)\r\n",
    "        \r\n",
    "        #- Step 2: If the dot product is bigger than or equal to zero, return 1. Otherwise, return 0.\r\n",
    "        \r\n",
    "        if act >= 0:\r\n",
    "            return 1\r\n",
    "        else: return 0\r\n",
    "        \r\n",
    "    def score(self, X, y):\r\n",
    "        \"\"\"\r\n",
    "        A method to calculate an accuracy score of a received dataset X and labels Y.\r\n",
    "\r\n",
    "        Input:\r\n",
    "          X: Dataset that we want to calculate scores.\r\n",
    "          y: True labels for the dataset X.\r\n",
    "\r\n",
    "        Output:\r\n",
    "          score: An accuracy with a range of [0, 1].\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "        \r\n",
    "        #- Step 1: Set the initial loss value to zero.\r\n",
    "        \r\n",
    "        loss = 0\r\n",
    "        \r\n",
    "        #- Step 2: We traverse all rows in our dataset and predict the label of each row\r\n",
    "        #          by calling self.predict method. If the label is different (prediction was wrong),\r\n",
    "        #          we add one to the loss value.\r\n",
    "        for idx, row in X.iterrows():\r\n",
    "            prediction = self.predict(row)\r\n",
    "            \r\n",
    "            if (y[idx] - prediction) != 0:\r\n",
    "                loss += 1\r\n",
    "        \r\n",
    "        #- Step 3: Calculate the accuracy score: Divide the summed loss value by the size of the dataset\r\n",
    "        #          and substract it from one.\r\n",
    "        \r\n",
    "        accuracy = 1 - loss/len(y)\r\n",
    "        \r\n",
    "        #- Step 4: Return the accuracy score.\r\n",
    "        return accuracy\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ANSWER"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Perceptron():\r\n",
    "    def __init__(self, max_iter):\r\n",
    "        self.max_iter = max_iter\r\n",
    "        \r\n",
    "    def fit(self, X, y):\r\n",
    "        self.w = np.zeros(len(X.iloc[0])+1)\r\n",
    "        for it in range(self.max_iter):\r\n",
    "            loss = 0\r\n",
    "            for idx, row in X.iterrows():\r\n",
    "                prediction = self.predict(row)\r\n",
    "                error = y[idx] - prediction\r\n",
    "                loss += error\r\n",
    "                self.w[0] = self.w[0] + error\r\n",
    "                self.w[1:] = self.w[1:] + error * row\r\n",
    "                \r\n",
    "    def predict(self, row):\r\n",
    "        act = self.w[0]\r\n",
    "        act += self.w[1:].dot(row)\r\n",
    "        if act >= 0:\r\n",
    "            return 1\r\n",
    "        else: return 0\r\n",
    "    \r\n",
    "    def score(self, X, y):\r\n",
    "        loss = 0\r\n",
    "        for idx, row in X.iterrows():\r\n",
    "            prediction = self.predict(row)\r\n",
    "            error = y[idx] - prediction\r\n",
    "            loss += abs(error)\r\n",
    "        return 1 - loss/len(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are done with implementation! Then we can create the instance, train the model, and test it in the same way!\n",
    "\n",
    "Run the codes below to check the result of the algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p = Perceptron(max_iter=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p.fit(X_train, y_train_numeric)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's check the score is the same as the one we got from scikit-learn that we already tried in the lab."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p.score(X_test, y_test_numeric)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### kNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it is kNN's turn. To implement kNN easier, we may need **Counter**, one of the built-in data structures in Python collections. It is okay if you do not know it, but it makes your job much easier! If you want to know it, refer to Python's official document [here](https://docs.python.org/3/library/collections.html#counter-objects)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the same structure again so that we can test in the same way!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class KNN:\r\n",
    "    def __init__(self, n_neighbors):\r\n",
    "        \"\"\"\r\n",
    "        A constructor that receives parameters and save them into member variables.\r\n",
    "        You will receive n_neighbors value and need to save into self.n_neighbors.\r\n",
    "\r\n",
    "        Input:\r\n",
    "          n_neighbors: Number of neighbors to look when we predict.\r\n",
    "        Output:\r\n",
    "          None.\r\n",
    "        \"\"\"\r\n",
    "        self.n_neighbors = None\r\n",
    "        \r\n",
    "        return\r\n",
    "        \r\n",
    "    def fit(self, X, y):\r\n",
    "        \"\"\"\r\n",
    "        A method to train the model by receiving the training dataset and labels.\r\n",
    "\r\n",
    "        - Step 1: Since there is no training process in KNN algorithm, we can just save the dataset and labels into\r\n",
    "                  the member variables self.X, self.y.\r\n",
    "                  \r\n",
    "        Input:\r\n",
    "          X: Training dataset.\r\n",
    "          y: Training labels.\r\n",
    "\r\n",
    "        Output:\r\n",
    "          None.\r\n",
    "        \"\"\"\r\n",
    "        self.X = None\r\n",
    "        self.y = None\r\n",
    "        \r\n",
    "        return None\r\n",
    "    \r\n",
    "    def euclidean_dist(self, d1, d2):\r\n",
    "        \"\"\"\r\n",
    "        A method to calculate an euclidean distance between two data points d1 and d2.\r\n",
    "\r\n",
    "        - Step 1: We calculate an euclidean distance, by substracting one point from the other, \r\n",
    "                  square it and take a squared root.\r\n",
    "                  \r\n",
    "        Input:\r\n",
    "          d1, d2: Data points (rows) from the dataset.\r\n",
    "          \r\n",
    "        Output:\r\n",
    "          distance: An euclidean distance value between d1 and d2.\r\n",
    "        \"\"\"\r\n",
    "        euclidean = None\r\n",
    "        return None\r\n",
    "        \r\n",
    "    def predict(self, row):\r\n",
    "        \"\"\"\r\n",
    "        A method to predict a label with trained weights.\r\n",
    "\r\n",
    "        Input:\r\n",
    "          row: A single row from dataset.\r\n",
    "        Output:\r\n",
    "          Binary integer (0 or 1).\r\n",
    "        \"\"\"\r\n",
    "        \r\n",
    "        #- Step 1: For a given row, we need to calculate distances from all data points of the training set.\r\n",
    "        # - To do this, create an empty list to save distances\r\n",
    "        distances = []\r\n",
    "        \r\n",
    "        #- Step 2: We iterate all training datasets and get [n_neighbors] nearest data points by calculating\r\n",
    "        #          euclidean distances from the input data point to all training datasets.\r\n",
    "        \r\n",
    "        # - iterate every row in the training set. We also need indices to recognize the rows.\r\n",
    "        for idx, row_train in self.X.iterrows():\r\n",
    "            # - calculate distance between the given row (row) and chosen row in the loop.\r\n",
    "            dist = self.euclidean_dist(row, row_train)\r\n",
    "            # - append the calculated distance to the list\r\n",
    "            distances.append((idx, dist))\r\n",
    "        \r\n",
    "        # sort the distances by the distance values, so we can get top k nearest neighbors\r\n",
    "        distances.sort(key=lambda x: x[1])\r\n",
    "        \r\n",
    "        #- Step 2: We will use the sorted list to get the labels from self.y \r\n",
    "        #          with the indices of [n_neighbors] nearest data points from self.X\r\n",
    "        #          and perform majority vote on [n_neighbors] nearest data points' labels. \r\n",
    "        #           In this stage, you can use collections.Counter to make this task easier.\r\n",
    "        \r\n",
    "        # - Create a list to keep the labels (y) of the chosen nearest neighbors\r\n",
    "        neighbors = []\r\n",
    "        \r\n",
    "        # - loop n_neighbors times and get first k (n_neighbors) labels using self.y\r\n",
    "        for i in range(self.n_neighbors):\r\n",
    "            neighbors.append(self.y[distances[i][0]])\r\n",
    "        \r\n",
    "        # - Step 3: Return the label that majority of the data points have.\r\n",
    "        # - Use Counter.most_common to return the most common label.\r\n",
    "        final_guess = Counter(neighbors).most_common(1)[0][0]\r\n",
    "        \r\n",
    "        return final_guess\r\n",
    "    \r\n",
    "    def score(self, X, y):\r\n",
    "        \"\"\"\r\n",
    "        A method to calculate an accuracy score of a received dataset X and labels Y.\r\n",
    "\r\n",
    "        Input:\r\n",
    "          X: Dataset that we want to calculate scores\r\n",
    "          y: True labels for the dataset X\r\n",
    "\r\n",
    "        Output:\r\n",
    "          score: An accuracy with a range of [0, 1]\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "        \r\n",
    "        #- Step 1: Set the initial loss value to zero.\r\n",
    "        loss = None\r\n",
    "        \r\n",
    "        #- Step 2: We traverse all rows in our dataset and predict the label of each row \r\n",
    "        #          by calling self.predict method. \r\n",
    "                \r\n",
    "        for idx, row in X.iterrows():\r\n",
    "            prediction = self.predict(row)\r\n",
    "            # - If the label is different (prediction was wrong), we add one to the loss value.\r\n",
    "            error = 1 if y[idx] == prediction else 0\r\n",
    "            loss += abs(error)\r\n",
    "        # - Step 3: Divide the loss value by a size of the dataset and substract it from one to get an accuracy score.\r\n",
    "        accuracy = loss/len(y)\r\n",
    "        # - Step 4: Return the accuracy score.\r\n",
    "        return loss/len(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ANSWER"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class KNN:\r\n",
    "    def __init__(self, n_neighbors):\r\n",
    "        self.n_neighbors = n_neighbors\r\n",
    "        \r\n",
    "    def fit(self, X, y):\r\n",
    "        self.X = X\r\n",
    "        self.y = y\r\n",
    "    \r\n",
    "    def euclidean_dist(self, d1, d2):\r\n",
    "        #print(np.sqrt(np.sum((d1 - d2)**2)))\r\n",
    "        return np.sqrt(np.sum((d1 - d2)**2))\r\n",
    "        \r\n",
    "    def predict(self, row):\r\n",
    "        distances = []\r\n",
    "        for idx, d2 in self.X.iterrows():\r\n",
    "            dist = self.euclidean_dist(row, d2)\r\n",
    "            distances.append((idx, dist))\r\n",
    "        \r\n",
    "        distances.sort(key=lambda x: x[1])\r\n",
    "        neighbors = []\r\n",
    "        for i in range(self.n_neighbors):\r\n",
    "            neighbors.append(self.y[distances[i][0]])\r\n",
    "        \r\n",
    "        final_guess = Counter(neighbors).most_common(1)[0][0]\r\n",
    "        return final_guess\r\n",
    "    \r\n",
    "    def score(self, X, y):\r\n",
    "        loss = 0\r\n",
    "\r\n",
    "        for idx, row in X.iterrows():\r\n",
    "            prediction = self.predict(row)\r\n",
    "            error = 1 if y[idx] == prediction else 0\r\n",
    "            loss += abs(error)\r\n",
    "        return loss/len(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's test it and see if it returns the same score on our test dataset!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "knn = KNN(n_neighbors = 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "knn.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's check the score is the same as the one we got from scikit-learn."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "knn.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# END OF LAB 3"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}