{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 2: Unsupervised Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The focus of this lab is to present the tools for conducting Unsupervised learning and specifically clustering using Python.\r\n",
    "We will implement Kmeans and use sklearn to apply K-medoids, Kmeans, Hierarchical clustering and several clustering evaluation metrics. Linked with the topic of this lab is HW2. \r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Outline:**\r\n",
    "\r\n",
    "1. K-means (sklearn)\r\n",
    "2. K-means algorithm implementation \r\n",
    "3. K-means caveats\r\n",
    "4. K-medoids\r\n",
    "5. Evaluation metrics\r\n",
    "\r\n",
    "6. Clustering on iris flower dataset\r\n",
    "\r\n",
    "\r\n",
    "    -  K-means, K-medoids, **Agglomerative Clustering**, Evaluation metrics and plotting with high-dimensional data\r\n",
    "    -  On Agglomerative Clustering: **Dendrogram** visualization with scipy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#importing libraries \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns \r\n",
    "import numpy as np \r\n",
    "\r\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris\r\n",
    "from sklearn.metrics import pairwise_distances_argmin\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Create some  Data\r\n",
    "#make_blobs( )generates  Gaussian blobs ideal for clustering.\r\n",
    "#link: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\r\n",
    "#the random state determines a random number generation for dataset creation. Essentially it generates a seed so that your results is always deterministic\r\n",
    "\r\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\r\n",
    "                       cluster_std=0.60, random_state=0)\r\n",
    "\r\n",
    "#scatterplot to visualize the blobs\r\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c=y_true ,cmap='rainbow');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#y_true are the integer labels for cluster membership of each sample\r\n",
    "y_true"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Kmeans (sklearn)\n",
    "\n",
    "K Means Clustering is an unsupervised learning algorithm that tries to cluster data based on their similarity. Unsupervised learning means that there is no outcome to be predicted, and the algorithm just tries to find patterns in the data. In k means clustering, we have the specify the number of clusters we want the data to be grouped into. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By eye, it is relatively easy to pick out the four clusters. The k-means algorithm does this automatically"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# In general, scikit-learn has a pretty comprehensive user-guide. You should always advice the user page of the algorithm that you are using in order to understand its functionality.\r\n",
    "# Here is the link for the Kmeans algorithm: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\r\n",
    "\r\n",
    "# Initialize a KMeans object, parameters of KMeans include the number of clusters and the initialization method\r\n",
    "\r\n",
    "# Method for initialization: default is kmeans++\r\n",
    "\r\n",
    "#‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. \r\n",
    "\r\n",
    "#‘random’: choose n_clusters observations (rows) at random from data for the initial centroids.\r\n",
    "\r\n",
    "# the blobs that we made had 4 centers, so we will look for 4 clusters!\r\n",
    "kmeans = KMeans(n_clusters=4)\r\n",
    "\r\n",
    "# compute kmeans clustering\r\n",
    "kmeans.fit(X)\r\n",
    "# call the kmeans object, predict, to predict the cluster index of each sample. \r\n",
    "\r\n",
    "# storing the predicted class memberships in y_kmeans\r\n",
    "y_kmeans = kmeans.predict(X)\r\n",
    "\r\n",
    "# in one step: kmeans.fit_predict(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# numpy array of the predicted index or labels for each cluster. As we asked for 4 clusters, the clusters are numbered from 0 to 3. \r\n",
    "y_kmeans"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize the results by plotting the data colored by these labels. We will also plot the cluster centers as determined by the k-means estimator:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the parameter c will map the sequence of numbers in y_kmeans (the cluster memberships) to colors\r\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\r\n",
    "\r\n",
    "# the cluster centers are stored as an attribured of the kmeans object. It will give you the coordinates of the cluster centers\r\n",
    "centers = kmeans.cluster_centers_\r\n",
    "\r\n",
    "# so we can plot those too! and make them black so they stand out\r\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.9);\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# you can see the coordinated of the centers \r\n",
    "centers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The good news is that the k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. K-Means implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# K-means is very straightforward and easy to implement. This is an example of Kmeans implementation. \r\n",
    "# The initial centers are chosen randomly from the data samples. \r\n",
    "from IPython.display import clear_output\r\n",
    "\r\n",
    "def kmeans(X, n_clusters, rseed=2):\r\n",
    "    \"\"\"\r\n",
    "    Input:\r\n",
    "        X: np array, the dataset to be clustered\r\n",
    "        n_clusters: number of clusters we want our data to be grouped into\r\n",
    "\r\n",
    "    Output:\r\n",
    "        centers: np array, the centers coordinates\r\n",
    "        labels: np array, the cluster memberships as found by Kmeans\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "\r\n",
    "    # 1. Randomly choose clusters\r\n",
    "\r\n",
    "    rng = np.random.RandomState(rseed) # generate a random number\r\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters] #get the n_cluster first elements from a randomly permulated sequence with length equal to the number of rows in X  \r\n",
    "    centers = X[i] # retrieve from X the elements with index as found above and make those as our first random centers\r\n",
    "\r\n",
    "    # Now that we generated the random centers we need to assign each point to one of these centers. \r\n",
    "    # To do that we need to calculate the distances between points and their closest centers. we can use sklearns pairwise_distances_argmin method for that.\r\n",
    "    # link: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html\r\n",
    "    \r\n",
    "    while True:\r\n",
    "        # 2a. Assign labels based on closest center\r\n",
    "\r\n",
    "        # This function computes for each row in X, \r\n",
    "        # the **index** of the row of centers which is closest (according to the specified distance).\r\n",
    "        # as a parameter it also gets a metric to use for distance computation\r\n",
    "        # the default is euclidean and this is what we are going to use here as we are implementing Kmeans\r\n",
    "        \r\n",
    "        labels = pairwise_distances_argmin(X, centers)\r\n",
    "        \r\n",
    "\r\n",
    "\r\n",
    "        # 2b. Find new centers from means of points\r\n",
    "\r\n",
    "        # Next we need to find the **new center** of each cluster and we \r\n",
    "        # can do that by calculating the mean of points of each cluster\r\n",
    "\r\n",
    "\r\n",
    "        new_centers = []\r\n",
    "        for i in range(n_clusters):\r\n",
    "            updated_center = (X[labels == i].mean(0))\r\n",
    "            new_centers.append(updated_center)\r\n",
    "            \r\n",
    "        new_centers = np.array(new_centers)\r\n",
    "\r\n",
    "        # here in one step \r\n",
    "        # new_centers = np.array([X[labels == i].mean(0)\r\n",
    "                                #for i in range(n_clusters)])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "        # 2c. Check for convergence\r\n",
    "        if np.all(centers == new_centers): # Test whether all array elements evaluate to True.\r\n",
    "            break\r\n",
    "        centers = new_centers\r\n",
    "        \r\n",
    "\r\n",
    "        fig, ax = plt.subplots()\r\n",
    "        clear_output(wait=True)\r\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\r\n",
    "        # plot the centroids also\r\n",
    "        ax.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.9)\r\n",
    "        plt.pause(0.5)\r\n",
    "        plt.show()\r\n",
    "        \r\n",
    "\r\n",
    "    return centers, labels\r\n",
    "\r\n",
    "    \r\n",
    "# call the K-means custom function\r\n",
    "centers, labels = kmeans(X, 4)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. CAVEATS of K-means\r\n",
    "1. If we use a different random seed in our simple procedure, the particular starting guesses may lead to poor results (try to change the random seed to 3 in the method that we created above)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "2. Another common challenge with k-means is that you must tell it how many clusters you expect: it cannot learn the number of clusters from the data. For example, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "3. Because each iteration of k-means must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows. \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "4. The fundamental model assumptions of k-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries. In particular, the boundaries between k-means clusters will always be linear, which means that it will fail for more complicated boundaries. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# To demonstrate bullet point 4:\r\n",
    "# This dataset from sklearn makes two interleaving half circles\r\n",
    "\r\n",
    "from sklearn.datasets import make_moons\r\n",
    "circles, y_circles = make_moons(200, noise=.05, random_state=0)\r\n",
    "\r\n",
    "plt.scatter(circles[:, 0], circles[:, 1], c=y_circles,\r\n",
    "            s=50, cmap='viridis');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Run kmeans with n_clusters=2 and plot the results as we did before!\r\n",
    "\r\n",
    "# Kmeans completely failes to find the clusters as the boundaries here are not linear\r\n",
    "\r\n",
    "labels = KMeans(2, random_state=0).fit_predict(circles)\r\n",
    "plt.scatter(circles[:, 0], circles[:, 1], c=labels,\r\n",
    "            s=50, cmap='viridis');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Example of algorithm that would work on this dataset\r\n",
    "# DBSCAN \r\n",
    "from sklearn.cluster import DBSCAN\r\n",
    "\r\n",
    "clustering = DBSCAN(eps=0.3, min_samples=2).fit_predict(circles)\r\n",
    "\r\n",
    "plt.scatter(circles[:, 0], circles[:, 1], c=clustering,\r\n",
    "            s=50, cmap='viridis');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. K-medoids \r\n",
    "\r\n",
    "K-Medoids algorithm can be used with various other dissimilarity measures (e.g. cosine similarity) or any distance metrics, unlike K-Means which needs Euclidean distance metric to arrive at efficient solutions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#K-medoids is not included in sklearn so download:\r\n",
    "#pip install scikit-learn-extra\r\n",
    "from sklearn_extra.cluster import KMedoids\r\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# In this case we will use the manhattan metric; the manhattan metric is best for binary data\r\n",
    "# To run Kmedoids on your dataset you follow a similar procedure as we did with KMeans.\r\n",
    "# Link to kmedoids user guide: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html\r\n",
    "\r\n",
    "#Kmedoids object init\r\n",
    "kmedoids = KMedoids(n_clusters=4, random_state=0, metric=\"manhattan\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#fit_predict\r\n",
    "y_medoids = kmedoids.fit_predict(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_medoids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#the labels predicted are saved as an attribute of the object\r\n",
    "kmedoids.labels_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#plot the result of kmedoids\r\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_medoids, s=50, cmap='viridis');\r\n",
    "\r\n",
    "#the clusters centers are stored in an attribute called cluster_centers_\r\n",
    "centers = kmedoids.cluster_centers_\r\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.9);\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Evaluation metrics: External and Internal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given the knowledge of grounds truth labels with can use: adjusted rand index. If we do not have the class labels then we can use silhouette score. \r\n",
    "\r\n",
    "The choice of metric depends on what we are interested in investigating and on the existance of the class/ground truth. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import metrics\r\n",
    "import pandas as pd \r\n",
    "\r\n",
    "# given that we have the class labels of the dataset we can use the so called external measures\r\n",
    "# For example we can use Adjusted Rand Index \r\n",
    "# It has two parameters namely labels_true, which is ground truth class labels, and labels_pred, which are clusters labels to evaluate. \r\n",
    "\r\n",
    "# With no knowledge of the class, we use internal measures. \r\n",
    "# Silhouette score is an internal measure because the clustering is evaluated based on merely the data that\r\n",
    "# was used for the clustering, \r\n",
    "# The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\r\n",
    "\r\n",
    "\r\n",
    "# Rand index of 1 means that we have perfectly matching labelings \r\n",
    "# a silhouette score with a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\r\n",
    "\r\n",
    "# X: array-like of shape (n_samples, n_features), the dataset \r\n",
    "# metric: string, the metric to use when calculating distance between instances in the feature array X. Used only for silhouette_score.  \r\n",
    "# labels: the predicted labels \r\n",
    "print(\"Silhoette_score for Kmeans: \", metrics.silhouette_score(X=X, labels=y_kmeans, metric=\"euclidean\"))\r\n",
    "\r\n",
    "\r\n",
    "# labels_pred: the labels predicted by the algorithm, array-like \r\n",
    "# labels_true: the true labels of the dataset/ground truth, array-like. None if labels do not exist\r\n",
    "print(\"ARI for Kmeans: \", metrics.adjusted_rand_score(labels_true=y_true, labels_pred=y_kmeans))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "print(\"Silhoette_score for Kmedoid: \", metrics.silhouette_score(X=X, labels=y_medoids, metric=\"manhattan\"))\r\n",
    "print(\"ARI for Kmedoid: \", metrics.adjusted_rand_score(labels_true=y_true, labels_pred=y_medoids))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Clustering on Iris Flower Dataset\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "The Iris Dataset contains four features (length and width of sepals and petals) of 50 samples of three species of Iris (Iris setosa, Iris virginica and Iris versicolor).\r\n",
    "Stored in a 150x4 numpy.ndarray\r\n",
    "\r\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "iris = load_iris()\r\n",
    "\r\n",
    "X = iris.data\r\n",
    "y = iris.target\r\n",
    "target_names = iris.target_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iris.feature_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Lets see how our dataset looks like\r\n",
    "\r\n",
    "# Plotting multi-dimensional datasets with more than two attributes is not as trivial. \r\n",
    "# You can always reduce the dimensionality of the feature space by applying PCA on the dataset and plot the resulting principal components. \r\n",
    "\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "import seaborn as sns \r\n",
    "\r\n",
    "pca = PCA(n_components=2).fit(X)\r\n",
    "pca_2d = pca.transform(X)\r\n",
    "\r\n",
    "\r\n",
    "sns.scatterplot(x=pca_2d[:, 0],y=pca_2d[:, 1],hue=y)\r\n",
    "plt.title(\"Iris Visualization\")\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE that if your variables are of incomparable units, before clustering you need to standardize them! Example: some variables are in cm and some other are in kg. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-means on iris"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE that if you would like to find the optimal number of clusters for your dataset you could run the elbow method for your dataset. \r\n",
    "## Kmeans++ init  (the default one)\r\n",
    "\r\n",
    "kmeans_iris = KMeans(n_clusters=3)\r\n",
    "kmeans_labels = kmeans_iris.fit_predict(X)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dictionary to store the predicted cluster memberships of the algorithms\r\n",
    "labels_dict = {}\r\n",
    "labels_dict[\"kmeans\"] = kmeans_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-medoids on iris"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run also KMedoids\r\n",
    "kmedoids = KMedoids(n_clusters=3, metric='euclidean')\r\n",
    "kmedoids_labels = kmedoids.fit_predict(X)\r\n",
    "labels_dict[\"kmedoids\"] = kmedoids_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agglomerative Clustering (on iris)\r\n",
    "\r\n",
    "\r\n",
    "Agglomerative clustering refers to a collection of algorithms  that build upon the same principle: the algorithm starts by declaring each point its own cluster, and then merges the two most similar clusters until some stopping criterion is satisfied. The stopping criterion in sklearn version is the number of clusters. \r\n",
    "What exactly is the most similar cluster is measured by several linkage criteria (sklearn choices: ward, average, complete). \r\n",
    "ward works on most datasets, but if the clusters have dissimilar members (if one is much bigger than all others, for example) then average/complete might work better. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import Agglomerative Clustering\r\n",
    "from sklearn.cluster import AgglomerativeClustering\r\n",
    "\r\n",
    "\r\n",
    "# Ward is the default linkage algorithm, so we'll start with that\r\n",
    "# Ward picks two clusters to merge such that the variance within all clusters increases the least. This often leads to clusters that are equally sized.\r\n",
    "\r\n",
    "# Agglomerative clustering works in the same way as the other algorithms from sklearn that we have seen so far. However, because of the way the algorithm works. \r\n",
    "# agglomerative clustering cannot make predictions for new data points. \r\n",
    "# Therefore, AgglomerativeClustering has no predict method. To build the model and get the cluster memberships on the dataset, use the fit_predict method instead. \r\n",
    "\r\n",
    "ward = AgglomerativeClustering(n_clusters=3)\r\n",
    "ward_labels = ward.fit_predict(X)\r\n",
    "labels_dict[\"ward\"] = ward_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "Let's also try complete and average linkages\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create an instance of AgglomerativeClustering with the appropriate parameters\r\n",
    "# complete linkage merges the two clusters that have the smallest maximum distance between their points \r\n",
    "complete = AgglomerativeClustering(n_clusters=3, linkage='complete')\r\n",
    "# Fit & predict\r\n",
    "complete_labels = complete.fit_predict(X)\r\n",
    "labels_dict[\"complete\"] = complete_labels\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# using average linkage\r\n",
    "# average linkage merges the two clusters that have the smallest average distance between all their points\r\n",
    "avg = AgglomerativeClustering(n_clusters=3, linkage='average')\r\n",
    "# fit and predict the cluster membership labels\r\n",
    "avg_labels = avg.fit_predict(X)\r\n",
    "labels_dict[\"avg\"] = avg_labels\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare the algorithms based on metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluation_metrics(X, labels_predicted, labels_true, metric):\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    Input: \r\n",
    "        X: array-like, the dataset\r\n",
    "        labels_predicted: array-like, the labels predicted by the algorithm \r\n",
    "        labels_true: array-like, ground truth, pass None if you don't have the ground truth labels\r\n",
    "        metric: metric to use for the calculation of the silhouette score, ex. 'euclidean', 'manhattan'\r\n",
    "    Output:\r\n",
    "        ari: the adjusted rand index value\r\n",
    "        ss: the silhouette score value\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "    ari = metrics.adjusted_rand_score(labels_true=labels_true, labels_pred=labels_predicted)\r\n",
    "    ss = metrics.silhouette_score(X=X, labels=labels_predicted, metric=metric)\r\n",
    "\r\n",
    "    return ari, ss\r\n",
    "\r\n",
    "\r\n",
    "for keys, values in labels_dict.items():\r\n",
    "    print(\"Clustering Algorithm: \", keys)\r\n",
    "    \r\n",
    "    ari, ss = evaluation_metrics(X, values, y, 'euclidean')\r\n",
    "    print(\"Silhouette Score: \", np.round(ss, decimals=3) , \"ARI: \", np.round(ari, decimals=3))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization, compare the clustering results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# most of the seaborn plots return a matplotlib axes object. So we can use the subplots() function to plot subplots with seaborn.\r\n",
    "\r\n",
    "f, axes = plt.subplots(1, 4, sharey=True, figsize=(15, 6))\r\n",
    "\r\n",
    "\r\n",
    "sns.scatterplot(x=pca_2d[:, 0],y=pca_2d[:, 1], hue=y, ax=axes[0])\r\n",
    "axes[0].set_title(\"Iris Original\" )\r\n",
    "\r\n",
    "sns.scatterplot(x=pca_2d[:, 0],y=pca_2d[:, 1],hue=kmeans_labels, ax=axes[1])\r\n",
    "axes[1].set_title(\"Iris Kmeans\")\r\n",
    "\r\n",
    "sns.scatterplot(x=pca_2d[:, 0],y=pca_2d[:, 1],hue=kmedoids_labels, ax=axes[2])\r\n",
    "axes[2].set_title(\"Iris Kmedoids\")\r\n",
    "\r\n",
    "sns.scatterplot(x=pca_2d[:, 0],y=pca_2d[:, 1],hue=avg_labels, ax=axes[3])\r\n",
    "axes[3].set_title(\"Iris Agglomerative (Avg)\")\r\n",
    "\r\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On Agglomerative Clustering: Dendrogram visualization with scipy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "The nice thing about hierarchical clustering is that it provides a complete dendrogram illustrating the relationships between clusters in our data. \r\n",
    "Unfortunately, scikit-learn currently does not have the functionality to draw dendrograms easily. But we can generate it easily using SciPy. \r\n",
    "\r\n",
    "\r\n",
    "Let's visualize the highest scoring clustering result.\r\n",
    "\r\n",
    "To do that, we'll need to use Scipy's linkage function to perform the clustering again so we can obtain the linkage matrix it will later use to visualize the hierarchy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The scipy clustering algorithms have a slightly different interface to the scikit-learn clustering algorithms. SciPy provides a function that takes a data array X and computes a linkage array, which \r\n",
    "# encodes the cluster similarities. We can then feed this linkage array into the scipy dendrogram function to plot the dendrogram. \r\n",
    "\r\n",
    "# Import scipy's linkage function to conduct the clustering\r\n",
    "from scipy.cluster.hierarchy import linkage\r\n",
    "\r\n",
    "# Specify the linkage type. Scipy accepts 'ward', 'complete', 'average', as well as other values\r\n",
    "# Pick the one that resulted in the highest Adjusted Rand Score\r\n",
    "linkage_type = 'average'\r\n",
    "\r\n",
    "linkage_matrix = linkage(X, linkage_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot using scipy's dendrogram function\r\n",
    "from scipy.cluster.hierarchy import dendrogram\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "plt.figure(figsize=(22,18))\r\n",
    "\r\n",
    "# plot for the linkage_matrix using 'dendrogram()'\r\n",
    "dendrogram(linkage_matrix)\r\n",
    "\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\r\n",
    "# On the x axis you see labels which are the indices of your samples in X. On the y axis you see the distances (of the 'average' method in our case).\r\n",
    "\r\n",
    "# Starting from each label at the bottom, you can see a vertical line up to a horizontal line. \r\n",
    "# The height of that horizontal line tells you about the distance at which this label was \r\n",
    "# merged into another label or cluster. You can find that other cluster by following the other vertical line down again. \r\n",
    "# If you don't encounter another horizontal line, it was just merged with the other label you reach, otherwise it was merged into another cluster that was formed earlier."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use a dendogram  to choose a number of the cluster for our data. Remember, a dendrogram only shows us the hierarchy of our data; it does not exactly give us the most optimal number of clusters.\n",
    "\n",
    "\n",
    "In order to identify clusters, we can cut the dendrogram horizontaly. The height of the cut to the dendrogram controls the number of clusters obtained. we can choose the cut-off point that cut the tallest vertical line"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# END OF LAB 2"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "interpreter": {
   "hash": "4a3821e50dfd29f54393a38062d93a54d0c9d954cd67861638d013f261604981"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}